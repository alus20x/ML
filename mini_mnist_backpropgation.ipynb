{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f3e0f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "data_path = \"./data.csv\"\n",
    "data = list()\n",
    "\n",
    "with open(data_path) as csv_file:\n",
    "    reader = csv.reader(csv_file)\n",
    "    for line, line_data in enumerate(reader):\n",
    "        i = 0\n",
    "        row_count = 0\n",
    "        \n",
    "        for row, row_data in enumerate(line_data):\n",
    "            if line == 0:\n",
    "                if row_data:\n",
    "                    data.append([row_data])\n",
    "                    \n",
    "            if line == 1:\n",
    "                if not row_data:\n",
    "                    i += 1\n",
    "                    row_count = 0\n",
    "                    continue\n",
    "                    \n",
    "                if row_count == 0:\n",
    "                    data[i].append([row_data])\n",
    "                    row_count += 1\n",
    "                    continue\n",
    "                \n",
    "                if row_count == 1 or row_count == 2:\n",
    "                    data[i][1].append(row_data)\n",
    "                    row_count += 1\n",
    "                    continue\n",
    "                \n",
    "            if line == 2:\n",
    "                if not row_data:\n",
    "                    i += 1\n",
    "                    continue\n",
    "                    \n",
    "                data[i][1].append(row_data)\n",
    "                \n",
    "            if line == 3:\n",
    "                if not row_data:\n",
    "                    i += 1\n",
    "                    continue\n",
    "                    \n",
    "                data[i][1].append(row_data)\n",
    "                \n",
    "            if line == 4:\n",
    "                if not row_data:\n",
    "                    i += 1\n",
    "                    continue\n",
    "                    \n",
    "                data[i][1].append(row_data)\n",
    "\n",
    "            if line == 5:\n",
    "                if row_data:\n",
    "                    if row_data == '0':\n",
    "                        data[i].append([1, 0])\n",
    "                    elif row_data == '1':\n",
    "                        data[i].append([0, 1])\n",
    "                    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b107da55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5922095a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/soullee/PycharmProjects/mlp/venv/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-14 00:08:46.433713: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-14 00:08:46.464776: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape 64\n",
      "0 1.2298939\n",
      "1000 0.0033221412\n",
      "2000 0.0014237069\n",
      "3000 0.00089472794\n",
      "4000 0.0006493789\n",
      "5000 0.00050843315\n",
      "6000 0.0004171629\n",
      "7000 0.00035332964\n",
      "8000 0.0003062319\n",
      "9000 0.00027006632\n",
      "10000 0.00024144765\n",
      "\n",
      "Hypothesis:  [[9.9984801e-01 1.4586814e-04]\n",
      " [9.9982941e-01 1.6226848e-04]\n",
      " [9.9983287e-01 1.6417154e-04]\n",
      " [9.9983460e-01 1.4510957e-04]\n",
      " [9.9980760e-01 1.9956924e-04]\n",
      " [9.9974597e-01 2.3616935e-04]\n",
      " [9.9932814e-01 5.1284319e-04]\n",
      " [9.9949181e-01 4.3780828e-04]\n",
      " [9.9934888e-01 5.0286279e-04]\n",
      " [9.9962497e-01 3.5692521e-04]\n",
      " [9.9981904e-01 1.8410097e-04]\n",
      " [9.9979132e-01 1.9619052e-04]\n",
      " [9.9969274e-01 3.6553096e-04]\n",
      " [9.9957168e-01 4.9370027e-04]\n",
      " [9.9955708e-01 4.2856985e-04]\n",
      " [9.9985254e-01 1.4457577e-04]\n",
      " [9.9981117e-01 1.9087597e-04]\n",
      " [9.9980658e-01 1.9926995e-04]\n",
      " [9.9985462e-01 1.4321615e-04]\n",
      " [9.9978870e-01 1.8506651e-04]\n",
      " [9.9980170e-01 1.8207714e-04]\n",
      " [9.9983716e-01 1.5512566e-04]\n",
      " [9.9977863e-01 2.2013045e-04]\n",
      " [9.9970245e-01 2.8124088e-04]\n",
      " [9.9984604e-01 1.4634921e-04]\n",
      " [9.9969798e-01 2.4174788e-04]\n",
      " [9.9975920e-01 2.1006848e-04]\n",
      " [9.9968064e-01 3.4527693e-04]\n",
      " [9.9972975e-01 2.9601666e-04]\n",
      " [9.9984848e-01 1.5497500e-04]\n",
      " [9.9971473e-01 2.6377244e-04]\n",
      " [9.9967539e-01 3.0706901e-04]\n",
      " [1.4416428e-04 9.9993026e-01]\n",
      " [1.3984462e-04 9.9991632e-01]\n",
      " [1.4416428e-04 9.9993026e-01]\n",
      " [1.4647415e-04 9.9991256e-01]\n",
      " [1.4609405e-04 9.9991536e-01]\n",
      " [1.4622632e-04 9.9990737e-01]\n",
      " [1.4164159e-04 9.9989361e-01]\n",
      " [1.5182643e-04 9.9989039e-01]\n",
      " [1.3793661e-04 9.9990815e-01]\n",
      " [1.4628812e-04 9.9990702e-01]\n",
      " [1.3802621e-04 9.9988860e-01]\n",
      " [1.4895732e-04 9.9988359e-01]\n",
      " [1.8657734e-04 9.9985403e-01]\n",
      " [1.6475600e-04 9.9987113e-01]\n",
      " [1.5172601e-04 9.9990475e-01]\n",
      " [1.9411449e-04 9.9975353e-01]\n",
      " [1.3984462e-04 9.9991632e-01]\n",
      " [4.5152882e-04 9.9907559e-01]\n",
      " [1.4169305e-04 9.9988025e-01]\n",
      " [1.4970859e-04 9.9989837e-01]\n",
      " [2.7667312e-04 9.9970895e-01]\n",
      " [1.3769350e-04 9.9987423e-01]\n",
      " [1.6322470e-04 9.9985868e-01]\n",
      " [1.5862168e-04 9.9989843e-01]\n",
      " [7.4161828e-04 9.9962980e-01]\n",
      " [1.5723734e-04 9.9989784e-01]\n",
      " [1.4580251e-04 9.9986833e-01]\n",
      " [1.6568280e-04 9.9988985e-01]\n",
      " [1.5342936e-04 9.9992055e-01]\n",
      " [1.3680159e-04 9.9990404e-01]\n",
      " [3.6308661e-04 9.9926400e-01]\n",
      " [1.5175693e-03 9.9867022e-01]] \n",
      "Correct:  [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "\n",
    "# Use tensorflow v1\n",
    "tf.disable_v2_behavior()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "learning_rate = 0.2\n",
    "\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for row in data:\n",
    "    x_data.append(row[1])\n",
    "    y_data.append(row[2])\n",
    "\n",
    "x_data = np.array(x_data, dtype=np.float32)\n",
    "y_data = np.array(y_data, dtype=np.float32)\n",
    "\n",
    "# Shape of image 3 * 4 = 12\n",
    "X = tf.placeholder(tf.float32, [None, 12], name=\"x\")\n",
    "Y = tf.placeholder(tf.float32, [None, 2], name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"Layer1\"):\n",
    "    W1 = tf.Variable(tf.truncated_normal([12, 20]), name='weight1')\n",
    "    b1 = tf.Variable(tf.truncated_normal([20]), name='bias1')\n",
    "    l1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "    \n",
    "    tf.summary.histogram(\"W1\", W1)\n",
    "    tf.summary.histogram(\"b1\", b1)\n",
    "    tf.summary.histogram(\"Layer1\", l1)\n",
    "    \n",
    "with tf.name_scope(\"Layer2\"):\n",
    "    W2 = tf.Variable(tf.truncated_normal([20, 20]), name='weight2')\n",
    "    b2 = tf.Variable(tf.truncated_normal([20]), name='bias2')\n",
    "    l2 = tf.sigmoid(tf.matmul(l1, W2) + b2)\n",
    "    \n",
    "    tf.summary.histogram(\"W2\", W2)\n",
    "    tf.summary.histogram(\"b2\", b2)\n",
    "    tf.summary.histogram(\"Layer2\", l2)\n",
    "\n",
    "with tf.name_scope(\"Layer3\"):\n",
    "    W3 = tf.Variable(tf.truncated_normal([20, 20]), name=\"weight3\")\n",
    "    b3 = tf.Variable(tf.truncated_normal([20]), name=\"bias3\")\n",
    "    l3 = tf.sigmoid(tf.matmul(l2, W3) + b3)\n",
    "        \n",
    "    tf.summary.histogram(\"W3\", W3)\n",
    "    tf.summary.histogram(\"b3\", b3)\n",
    "    tf.summary.histogram(\"Layer3\", l3)\n",
    "\n",
    "with tf.name_scope(\"Layer4\"):\n",
    "    W4 = tf.Variable(tf.truncated_normal([20, 2]), name=\"weight4\")\n",
    "    b4 = tf.Variable(tf.truncated_normal([2]), name=\"bias4\")\n",
    "    Y_pred = tf.sigmoid(tf.matmul(l3, W4) + b4)\n",
    "    \n",
    "    tf.summary.histogram(\"W4\", W4)\n",
    "    tf.summary.histogram(\"b4\", b4)\n",
    "    tf.summary.histogram(\"Layer4\", Y_pred)\n",
    "\n",
    "# cost/loss function\n",
    "with tf.name_scope(\"Cost\"):\n",
    "    cost = -tf.reduce_mean(Y * tf.log(Y_pred) + (1 - Y) * tf.log(1 - Y_pred))\n",
    "    tf.summary.scalar(\"Cost\", cost)\n",
    "\n",
    "# Loss derivative\n",
    "d_Y_pred = (Y_pred - Y) / (Y_pred * (1.0 - Y_pred) + 1e-7)\n",
    "\n",
    "# Layer4\n",
    "d_sigma4 = Y_pred * (1 - Y_pred)\n",
    "d_a4 = d_Y_pred * d_sigma4\n",
    "d_p4 = d_a4\n",
    "d_b4 = d_a4\n",
    "d_W4 = tf.matmul(tf.transpose(l3), d_p4)\n",
    "\n",
    "# Layer4-Mean\n",
    "d_b4_mean = tf.reduce_mean(d_b4, axis=[0])\n",
    "d_W4_mean = d_W4 / tf.cast(tf.shape(l3)[0], dtype=tf.float32)\n",
    "\n",
    "# Layer3\n",
    "d_l3 = tf.matmul(d_p4, tf.transpose(W4))\n",
    "d_sigma3 = l3 * (1 - l3)\n",
    "d_a3 = d_l3 * d_sigma3\n",
    "d_b3 = d_a3\n",
    "d_p3 = d_a3\n",
    "d_W3 = tf.matmul(tf.transpose(l2), d_a3)\n",
    "\n",
    "# Layer3-Mean\n",
    "d_W3_mean = d_W3 / tf.cast(tf.shape(l2)[0], dtype=tf.float32)\n",
    "d_b3_mean = tf.reduce_mean(d_b3, axis=[0])\n",
    "\n",
    "# Layer2\n",
    "d_l2 = tf.matmul(d_p3, tf.transpose(W3))\n",
    "d_sigma2 = l2 * (1 - l2)\n",
    "d_a2 = d_l2 * d_sigma2\n",
    "d_b2 = d_a2\n",
    "d_p2 = d_a2\n",
    "d_W2 = tf.matmul(tf.transpose(l1), d_a2)\n",
    "\n",
    "# Layer2-Mean\n",
    "d_W2_mean = d_W2 / tf.cast(tf.shape(l1)[0], dtype=tf.float32)\n",
    "d_b2_mean = tf.reduce_mean(d_b2, axis=[0])\n",
    "\n",
    "# Layer1\n",
    "d_l1 = tf.matmul(d_p2, tf.transpose(W2))\n",
    "d_sigma1 = l1 * (1 - l1)\n",
    "d_a1 = d_l1 * d_sigma1\n",
    "d_b1 = d_a1\n",
    "d_p1 = d_a1\n",
    "d_W1 = tf.matmul(tf.transpose(X), d_a1)\n",
    "\n",
    "# Layer1-Mean\n",
    "d_W1_mean = d_W1 / tf.cast(tf.shape(X)[0], dtype=tf.float32)\n",
    "d_b1_mean = tf.reduce_mean(d_b1, axis=[0])\n",
    "\n",
    "# Weight update\n",
    "step = [\n",
    "  tf.assign(W4, W4 - learning_rate * d_W4_mean),\n",
    "  tf.assign(b4, b4 - learning_rate * d_b4_mean),\n",
    "    \n",
    "  tf.assign(W3, W3 - learning_rate * d_W3_mean),\n",
    "  tf.assign(b3, b3 - learning_rate * d_b3_mean),\n",
    "    \n",
    "  tf.assign(W2, W2 - learning_rate * d_W2_mean),\n",
    "  tf.assign(b2, b2 - learning_rate * d_b2_mean),\n",
    "    \n",
    "  tf.assign(W1, W1 - learning_rate * d_W1_mean),\n",
    "  tf.assign(b1, b1 - learning_rate * d_b1_mean)\n",
    "]\n",
    "\n",
    "# Accuracy computation\n",
    "# Get the maximum value, and check the value is equal\n",
    "predicted = tf.equal(tf.argmax(Y_pred, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(predicted, dtype=tf.float32))\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # tensorboard --logdir=./logs/hand_writings_01\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"./logs/hand_writings_01\")\n",
    "    writer.add_graph(sess.graph)  # Show the graph\n",
    "    \n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    print(\"shape\", sess.run(tf.shape(X)[0], feed_dict={X: x_data}))\n",
    "\n",
    "    for i in range(10001):\n",
    "        _, summary, cost_val = sess.run([step, merged_summary, cost], feed_dict={X: x_data, Y: y_data})\n",
    "        writer.add_summary(summary, global_step=i)\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print(i, cost_val)\n",
    "\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([Y_pred, predicted, accuracy],\n",
    "                       feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f503894b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
